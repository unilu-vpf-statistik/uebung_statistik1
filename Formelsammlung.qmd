---
title: "Formelsammlung"
subtitle: "HS 2025"
author: "Gidon Frischkorn"
---

# Formelsammlung: VL Statistik 1

Die folgenden Formelsammlung gibt eine Übersicht über die mathematischen Formeln und Definitionen, die im Rahmen der Vorlesung "Statistik 1" eingeführt und behandelt wurden. 

Es wird erwartet, dass Sie mit den in der Formelsammlung aufgelisteten Formeln und Definitionen einfache Rechnungen durchführen können und diese mit Hilfe der Statistiksoftware R & RStudio, oder einem der für die Prüfung zugelassenen Taschenrechner auf einfach Rechenbeispiele wie in den Übungen anwenden können.

Im Laufe des Semesters werden Schritt für Schritt weitere Formeln hinzugefügt werden. 

## Grundlegenden Rechenoperatoren

### Summe

$$
\sum_{i = 1}^j x_j = x_1 + x_2 + x_3 + \cdots + x_j
$$

Die Summe alle Werte der Variablen $x$ begonnen ab dem Startwert $i = 1$ bis zu dem Endwert $j$.

## Deskriptive Statistik

### Häufigkeitsverteilung

#### Absolute Häufigkeit

$n_j =$ absolute Häufigkeit einer Kategorie $j$ wird mit, wobei: $\sum_{j=1}^k n_j = n$ die Gesamtzahl an Beobachtungen $n$ für eine Menge von $k$ Kategorien ergibt

#### Relative Häufigkeit

$$
h_j = \frac{n_j}{n}
$$

mit der absoluten Häufigkeit $n_j$ der Kategorie $j$ und der Gesamtzahl an Beobachtungen $n$

#### Kumulierte Häufigkeiten

$$
kn_j = \sum_{i=1}^j n_i
$$

Summe der absoluten Häufigkeiten $n_i$  von der kleinsten Kategorie $i = 1$ bis zur Kategorie $j$.

diese Summe kann analog auch für relative Häufikeiten berechnet werden:

$$
kh_j = \sum_{i=1}^j h_i = \frac{kn_j}{n} = \frac{\sum_{i=1}^j n_i}{n}
$$

### Masse der zentralen Tendenz

#### Modus

Der **Modus** (engl. Mode) ist der *häufigste* Werte einer primären Häufigkeitsverteilung.

Für kontinuierliche (metrische) Variablen ist der **Modus** das *Maximum* der Häufigkeitsdichte

#### Median



#### Mittelwert

$$
\overline{x} = \frac{\sum_{m = 1}^{n} x_m}{n} = M
$$ mit eine Gesamtzahl von $n$ Messweten $x_m$

### Dispersionsmasse

#### Relativer Informationsgehalt

$$
H = - \frac{1}{\ln{k}} \cdot \sum_{j = 1}^{k} h_j \cdot \ln{h_j}
$$

für eine Häufigkeitsverteilung mit $k$ unterschiedlichen Kategorien und den relativen Häufigkeiten $h_j$ für jede einzelne Kategorie $j$

#### Interquartilsabstand (bzw. Interquartilerange, IQR)

$$
IQR = Q_3 - Q_1
$$

mit: $$
Q_i = x_q; \quad \text{und:} \quad q = \frac{i}{4} \cdot n
$$

Für aufsteigend sortierte Messwerte $x$ gibt $q$ den Messwert an, der die Quartilgrenze des $i$-ten Quartils darstellt.

#### Mittlere Absolute Medianabweichung (MAD)

$$
MAD = \frac{\sum_{m=1}^n |x_m - Md_x|}{n}
$$ mit der Summe $\sum$ der absoluten Abweichungen der Messwerte $x_m$ vom Median $Md$ der Variable $x$ normiert an der Anzahl an Messwerten $n$

#### Varianz / Standardabweichung

die **Varianz** ergibt sich aus dem an der Anzahl an Messwerten $n$ normierten zentralen / zentrierten Moment *zweiter* Ordnung:

$$
s_x^2= \frac{\sum_{m = 1}^n (x_m - \overline{x})^2}{n}
$$

die **Standardabweichung** ergibt sich indem man die Quadratwurzel der Varianz zieht:

$$
s_x = \sqrt{s_x^2} = \sqrt{\frac{\sum_{m = 1}^n (x_m - \overline{x})^2}{n}}
$$

#### Schiefe

die **Schiefe** ergibt sich aus dem an der Anzahl an Messwerten $n$ normierten und der Standardabweichung $s_x$ standardisierten zentralen / zentrierten Moment *dritter* Ordnung:

$$
Sch = \frac{1}{n} \cdot \frac{\sum_{m=1}^n (x_m - \overline{x})^3}{s_x^3} = \frac{1}{n} \cdot \sum_{m=1}^n \left( \frac{x_m - \overline{x}}{s_x} \right)^3
$$

#### Stichprobenmomente

Das $k$-te Stichprobenmoment $m$ für eine Variable $X$ ist definiert als:

$$
m_k = \frac{\sum_{i = 1}^n X_i^k}{n}
$$

das zentrale oder zentrierte $k$-te Stichprobenmoment $m$ für eine Variable $X$ ist definiert als:

$$
m_k = \frac{\sum_{i = 1}^n (X_i - \overline{X})^k}{n}
$$

mit dem Mittelwert $\overline{X}$

## Wahrscheinlichkeitstheorie

### LaPlace Wahrscheinlichkeit

$$
P(A) = \frac{N_A}{K}
$$

für ein Zufallsexperiment bei dem die Wahrscheinlichkeit aller Elementarereignisse gleich ist (= LaPlace Experiment), mit der Anzahl für das Ereignis A günstigen Ergebnissen $N_A$, und der Gesamtzahl an Ergebnissen $K$ im Ergebnisraum $\Omega$ 

### Kombinatorik

#### *mit* Zurücklegen, *mit* Berücksichtigung der Reihenfolge

$$
K = k^n
$$
mit $k$ gleich der Anzahl der Ergebnisse für eine einzelne Ziehung, und $n$ Ziehungen aus der Urne

#### *ohne* Zurücklegen, *mit* Berücksichtigung der Reihenfolge

$$
K = \frac{k!}{(k-n)!}
$$

mit $x! = x \cdot (x-1) \cdot (x-2) \cdot \ldots \cdot (k -(k-1))$, gesprochen $x$-Fakultät. Zum Beispiel: $8! = 8 \cdot 7 \cdot 6 \cdot 5 \cdot \ldots \cdot 1 = 40320$

#### *ohne* Zurücklegen, *ohne* Berücksichtigung der Reihenfolge

$$
K = \frac{k!}{(k-n)! \cdot n!} = \binom{k}{n}
$$

ist der Binomialkoeffizient $\binom{k}{n}$, gesprochen $k$ über $n$

#### *mit* Zurücklegen, *ohne* Berücksichtigung der Reihenfolge

$$
K = \frac{(k + n -1)!}{(k-1)! \cdot n!}
$$

### Rechenregeln: Wahrscheinlichkeit

#### Wahrscheinlichkeit: Teilmengen

Ist das Ereignis $A$ eine Teilmenge des Ereignisses $B$, gilt:

$$
P(B) > P(A)
$$

#### Komplementäre Wahrscheinlichkeit

$$
P(\overline{A}) = 1 - P(A)
$$

mit der Wahrscheinllichkeit, dass das Ereignis A nicht eintritt $\overline{A}$

#### Wahrscheinlichkeit: Schnittmengen

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

### Bernoulli Theorem

$$
\lim_{n \rightarrow \infty} P(|h(A)-P(A)|< \epsilon) = 1
$$

geht die Anzahl an Beobachtungen $n$ gegen unendlich $\infty$, dann ist die Wahrscheinlichkeit, dass die absolute Differenz der relativen Häufigkeit von A $h(A)$ und der wahren Wahrscheinlichkeit von A $P(A)$ kleiner als ein beliebig kleiner Wert / Fehler $\epsilon$ gleich 1.

### Bedingte Wahrscheinlichkeit

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

### Stochastische Unabhängigkeit

zwei Ereignisse $A$ und $B$ sind stochastisch voneinander unabhängig, wenn entweder das Multiplikations-Theorem gilt:

$$
P(A \cap B) = P(A) \cdot P(B)
$$

oder die bedingte Wahrscheinlichkeit, gleich der unbedingten Wahrscheinlichkeit ist:

$$
P(B|A) = P(B)
$$

### Bayes Theorem

$$
P(A|B) = \frac{P(B|A) * P(A)}{P(B)}
$$

wobei:

$$
P(B) = P(B|A) * P(A) + P(B|\neg  A) * P(\neg  A)
$$

#### Odds (Chancen)

Odds (bzw. Chancen) lassen sich aus Wahrscheinlichkeiten berechnen:

$$
O = \frac{p}{1-p}; \quad p = \frac{O}{O+1}
$$

#### Bayes Faktor

Der Bayes Faktor gibt das Verhältnis der Wahrscheinlichkeiten bestimmter Daten $D$ unter unterschiedlichen Hypothesen $H_i$ an. Wenn Sie zum Beispiel einen medizinischen Test betrachten, dann können Sie den Bayes Faktor für die Wahrscheinlichkeite einer Krankheit $K^+$ geben der Test ist positiv $T^+$, folgenderemassen berechnen:

$$
BF_{K^+} = \frac{P(T^+|K^+)}{P(T^+|K^-)}
$$
den Bayes Faktor können Sie zur Aktualisierung der Prior Odds in die Posterior Odds nutzen:

$$
O_{Post(K^+)} = O_{Prior(K^+)} \cdot BF_{K^+} 
$$


### Wahrscheinlichkeitsverteilungen

#### Binomialverteilung

Die Wahrscheinlichkeitsfunktion der Binomialverteilung ist:

$$
f(x) = \binom{n}{x} \cdot \pi^x \cdot (1-\pi)^{n-x}
$$
wobei $n$ der Anzahl ziehungen entspricht, und $\pi$ die zugrunde liegenden Wahrscheinlichkeit für einen Erfolg $x$ darstellt.

Der Erwartungswert der Binomialverteilung ergibt sich aus:

$$
E(X) = n \cdot \pi
$$

und die Varianz ergibt sich aus:

$$
Var(X) = n \cdot \pi \cdot (1-\pi)
$$

mit zunehmenden $n$ und Wahrscheinlichkeiten abseits der extreme: $0.1 > \pi > 0.9$ nähert sich die Binomialverteilung der Normalverteilung an. Für sehr grosse $n > 250$ gilt das auch für extreme Wahrscheinlichkeiten $\pi < 0.1$ oder $\pi > 0.9$

#### Exponentialverteilung

Die Dichtefunktion einer Exponentialverteilung lautet für $x \geq 0$:

$$
f(x) = \lambda \cdot e^{-\lambda \cdot x}
$$

wobei $\lambda$ die Rate des Abfalls der Wahrscheinlichkeitsdichte bestimmt.

Der Erwartungswert der Binomialverteilung ergibt sich aus:

$$
E(X) = \frac{1}{\lambda}
$$

und die Varianz ergibt sich aus:

$$
VAR(X) = \frac{1}{\lambda^2}
$$

#### Normalverteilung

Die Dichtefunktion der Normalverteilung für $x \in \mathbb{R}$ lautet:

$$
f(x) = \frac{1}{\sigma \cdot \sqrt{2\pi}} \cdot e^{-\frac{1}{2}\cdot \left(\frac{x-\mu}{\sigma} \right)^2}
$$

mit dem Erwartungswert $\mu$, und der Varianz $\sigma^2$

Eine Normalverteilte Variable $X$ kann in eine Standardnormalverteilte Variable $Z$ mit dem Erwartungswert $\mu = 0$ und der Varianz $\sigma^2 = 1$ transformiert werden:

$$
Z = \frac{X - \mu}{\sigma}; \quad X_p = Z_p \cdot \sigma + \mu
$$



## Inferenzstatistik: Definitionen

### Hypothesentests

#### Nullhypothesentests: Fischer

**Ungerichtete** Nullhypothese: $H_0: \hat{\theta} = \theta_0$

$\rightarrow$ Der Populationsschätzer eines Paramters $\hat{\theta}$ unterscheidet sich nicht von einem Populationswert unter der Nullhypothese $\theta_0$. Typischerweise wird $\theta_0 = 0$ angenommen.

**Gerichtete** Nullhypothese: $H_0: \hat{\theta} \leq \theta_0$ oder $H_0: \hat{\theta} \geq \theta_0$

$\rightarrow$ Der Populationsschätzer eines Paramters $\hat{\theta}$ ist kleiner oder gleich bzw. grösser oder gleich einem Populationswert unter der Nullhypothese $\theta_0$. Typischerweise wird $\theta_0 = 0$ angenommen.

**Kritischer** Testwert

$\rightarrow$ Wert einer Teststatistik $T$ (z.Bsp: $z$ oder $t$), der gerade noch mit der Nullhypothese vereinbar ist. Für ein bestimmtest Signifikanzniveau $\alpha$ gilt:

$$
\text{ungerichtete Hypothese:} \quad |T_{krit}| = F^{-1}(p = 1-\alpha/2) 
$$

$$
\text{gerichtete Hypothese:} \quad |T_{krit}| = F^{-1}(p = 1-\alpha)
$$

mit der Quantilfunktion $F^{-1}$ der entsprechenden Verteilung der jeweiligen Teststatistik.

**$p$-Wert**

Wahrscheinlichkeit ein bestimmtes Ergebnis bzw. eine bestimmte empirische Teststatistik $T_emp$ oder extremere Ergebnisse unter Annahme der Nullhypothese zu beobachten.

$$
\text{ungerichtete Hypothese:} \quad p = 2\cdot (1 - F(|T_{emp})|) 
$$

$$
\text{gerichtete Hypothese:} \quad p = 1 - F(|T_{emp}|)
$$

mit der Verteilungsfunktion $F$ der entsprechenden Verteilung der jeweiligen Teststatistik.

#### Binäres Entscheidungskonzept: Neyman & Pearson

Nutzt die gleichen Hypothesen wie das Konzept des Nullhypothesentests von Fischer ergänzt diese aber mit einer komplementären Alternativhypothese $H_1$

bei **Ungerichteter** Nullhypothese, gilt: $H_1: \hat{\theta} \neq \theta_0$

$\rightarrow$ Der Populationsschätzer eines Paramters $\hat{\theta}$ unterscheidet sich von einem Populationswert unter der Nullhypothese $\theta_0$. Typischerweise wird $\theta_0 = 0$ angenommen.

bei **Gerichteter** Nullhypothese: $H_1: \hat{\theta} > \theta_0$ oder $H_0: \hat{\theta} < \theta_0$

$\rightarrow$ Der Populationsschätzer eines Paramters $\hat{\theta}$ ist kleiner bzw. grösser als der Populationswert unter der Nullhypothese $\theta_0$. Typischerweise wird $\theta_0 = 0$ angenommen.

**$\alpha$-Fehler** oder Fehler 1. Art

$\rightarrow$ Wahrscheinlichkeit mit der ich die Alternativhypothese annehme, obwohl in der Population die Nullhypothese gilt.

**$\beta$-Fehler** oder Fehler 2. Art

$\rightarrow$ Wahrscheinlichkeit mit der ich die Nullhypothese annehme, obwohl in der Population die Alternativhypothese gilt.

Der $\beta$-Fehler kann nur unter Annahme eines bestimmten Effekts $\epsilon$ bestimmt werden.


### Parameterschätzung

#### Zentraler Grenzwertsatz

Für eine Stichprobe von $N$ unabhängigen Werten einer Zufallsvariable $X$ aus einer beliebigen Wahrscheinlichkeitsverteilung $\mathcal{D}$ mit endlicher Varianz $(0 < \sigma^2 < \infty)$: $X \sim \mathcal{D}(\theta)$
gilt mit wachsendem $N$ $(N \rightarrow \infty)$ konvergiert:

a) die Summe der beobachteten Werte $\sum X$ zu einer Normalverteilung $\mathcal{N}$
b) mit dem Erwartungswert: $\mu_{\sum} = N \cdot \mu_{\mathcal{D}}$
c) und der Standardabweichung: $\sigma_{\sum} = \sqrt{N} \cdot \sigma_{\mathcal{D}}$

#### Standardfehler des Mittelwerts

Für eine Stichprobe mit $N$ Werten einer Normalverteilte Variable $X \sim \mathcal{N}(\mu_X, \sigma_X)$ ist der Standarfehler des Mittelwerts $\bar{X} = \frac{\sum X}{N}$:

$$
\sigma_{\bar{x}} = \frac{\sigma_X}{\sqrt{N}}
$$

Für eine Stichprobe mit $N$ Werte einer Zufallsvariable $Y$, die einer beliebigen Wahrscheinlichkeitsverteilung $\mathcal{D}$ mit endlicher Varianz ($0 < \sigma^2 < \infty$) folgt, konvergiert der Standardfehler des Mittelwerts $\bar{y} = \frac{\sum Y}{N}$ mit wachsender Stichprobengrösse $N \rightarrow \infty$ gegen:

$$
\sigma_{\bar{y}} = \frac{\sigma_Y}{\sqrt{N}}
$$

#### Standardfehler des Medians

Für eine Stichprobe mit $N$ Werten einer Normalverteilte Variable $X \sim \mathcal{N}(\mu_X, \sigma_X)$ ist der Standarfehler des Medians:

$$
\sigma_{Md} = 1.253 \cdot \frac{\sigma_X}{\sqrt{N}}
$$

#### Konfidenzintervalle

bei **bekannten** Populationsparametern einer Zufallsvariable $X$ aus einer Normalverteilten Variable: $X \sim \mathcal{N}(\mu_X, \sigma_X)$ berechnet sich das Konfidenzintervall, für die Konfidenz $(1-\alpha)$ mit Hilfe der $z$-Werte:

mit *auf beiden Seiten geschlossenen* Grenzen, berechnen sich die Grenzen:

$$
\mu  \pm z_{1-\alpha/2} * \sigma
$$

für *nach unten* oder *nach oben* geschlossene Grenzen, berechnet sich die Grenze:

$$
\mu  \pm z_{1-\alpha} * \sigma
$$

bei **unbekannten** Populationsparametern einer Zufallsvariable $X$ aus einer Normalverteilten Variable: $X \sim \mathcal{N}(\mu_X, \sigma_X)$ berechnet sich das Konfidenzintervall, für die Konfidenz $(1-\alpha)$ mit Hilfe der $t$-Werte aus $t(df = N-1,0,1)$ und der Stichprobengrösse $N$ aus der die Populationsparameter geschätzt werden:

mit *auf beiden Seiten geschlossenen* Grenzen, berechnen sich die Grenzen:

$$
\hat{\mu}  \pm t_{1-\alpha/2}(df = N-1) * \hat{\sigma} = \bar{x} \pm t_{1-\alpha/2}(df = N-1) \cdot \frac{s_X}{(N-1)}
$$

für *nach unten* oder *nach oben* geschlossene Grenzen, berechnet sich die Grenze:

$$
\hat{\mu}  \pm t_{1-\alpha}(df = N-1) * \hat{\sigma} = \bar{x} \pm t_{1-\alpha}(df = N-1) \cdot \frac{s_X}{(N-1)}
$$

## Gruppenvergleiche

### Metrische Variablen

#### Einstichproben $t$-Test

für eine Stichprobe mit $N$ Werten aus einer Normalverteilten Variable: $X \sim \mathcal{N}(\mu_X, \sigma_X^2)$, berechnet sich die Teststatistik des Einstichproben $t$-Tests:

$$
t_{emp} = \frac{\hat{\mu}_X - \mu_0}{\hat{\sigma}_{\hat{\mu}_X}}  = 
\frac{\bar{x} - \mu_0}{\hat{\sigma}_{\bar{x}}}
$$

mit:

$$
\sigma_{\bar{x}} = \frac{\hat{\sigma}_X}{\sqrt{N}}
$$

und den Freiheitsgraden $df = N-1$

#### $t$-Test für *unabhängige* Stichproben

für unabhängige Stichproben aus zwei Gruppen $i = 1,2$ mit $N_1 \& N_2$ Werten, die einer Normalverteilung folgen: $X_{ip} \sim \mathcal{N}(\mu_i, \sigma_i^2)$, berechnet sich die Teststatistik des $t$-Tests für unabhämngige Stichproben folgendermassen:

$$
t_{emp} = \frac{\hat{\mu}_1 - \hat{\mu}_2}{\hat{\sigma}_{\hat{\mu}_1 - \hat{\mu}_2}} = 
\frac{\bar{x_1} - \bar{x_2}}{\hat{\sigma}_{\bar{x}_1 - \bar{x}_2}}
$$

mit:

$$
\hat{\sigma}_{\bar{x}_1 - \bar{x}_2} = \sqrt{\frac{\hat{\sigma}_{inn}^2}{N_1} + \frac{\hat{\sigma}_{inn}^2}{N_2}}
$$

und: 

$$
\hat{\sigma}_{inn}^2 = \frac{\hat{\sigma}_1^2 \cdot(N_1 -1) + \hat{\sigma}_2^2 \cdot(N_2-1)}{(N_1 -1) + (N_2 - 1)}
$$


und den Freiheitsgraden $df = N_1 + N_2 - 2 = (N_1 -1) + (N_2 - 1)$

**Beachten Sie**: Die Reihenfolge mit der die Differenz der Mittelwerte berechnet wirkt ist in der Regel nicht relevant. Sie müssen nur darauf achten die gerichteten Hypothesen entsprechend zu spezifizieren.

#### $t$-Test für *abhängige* Stichproben

für abhängige Stichproben mit $N$ Werten, deren Differenz $X_D = X_1 - X_2$ einer Normalverteilung folgt: $X_D \sim \mathcal{N}(\mu_D, \sigma_D^2)$, berechnet sich die Teststatistik des $t$-Tests für abhängige Stichproben:

$$
t_{emp} = \frac{\hat{\mu}_D}{\hat{\sigma}_{\mu_D}} = \frac{\bar{x}_D}{\hat{\sigma}_{\bar{x}_D}}
$$

mit:

$$
\sigma_{\bar{x}_D} = \frac{\hat{\sigma}_{X_D}}{\sqrt{N}}
$$

und den Freiheitsgraden $df = N-1$

### Diskrete Variablen

#### $\chi^2$-Test: Eine Stichprobe

Für eine Häufigkeitsverteilung der beobachteten Häufigkeiten $n$ mit $j$ Kategorien:

$$
\chi^2 = \sum^j_{i = 1} \frac{(n_i - \epsilon_i)^2}{\epsilon_i}
$$

und den Freiheitsgraden: $df = j -1$

#### $\chi^2$-Test: Zwei Stichproben

Für eine Häufigkeitsverteilung von beobachteten Häufigkeiten $n$ mit $j$ Kategorien und $p$ Gruppen berechnet sich der empirische $\chi^2$ Wert:

$$
\chi^2 = \sum_{k = 1}^p \sum_{i = 1}^j \frac{(n_{ki}-\epsilon_{ki})^2}{\epsilon_{ki}}
$$

und den Freiheitsgraden: $df = (k-1) \cdot (j - 1)$

Unter der Annahme das die Merkmale der Kreuztabelle unabhängig sind berechnet sich die geschätzte erwartete Häufigkeit:

$$
\hat{\epsilon}_{ki} = \frac{n_k \cdot n_i}{N}
$$

## Fehler & Fragen

Sollten Ihnen Fehler in der Formelsammlung auffallen, dann prüfen Sie diese bitte zuerst mit den Folien der Vorlesung, sowie den in der Pflichtliteratur angegebenen Formeln ab. Wenn diese Prüfung Ihre Fehlervermutung bestätigt, dann schreiben Sie bitte eine Mail unter der Angabe der Formel an: [gidon.frischkorn\@unilu.ch](mailto:gidon.frischkorn@unilu.ch)
